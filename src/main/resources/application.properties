spring.application.name=aichat
server.port=8083

# Ollama Configuration for Llama 3 8B with performance optimizations
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.model=llama3:8b

# Performance-optimized parameters
spring.ai.ollama.chat.options.num-predict=-1
spring.ai.ollama.chat.options.num-ctx=8192

spring.ai.ollama.chat.options.num-batch=1024
spring.ai.ollama.chat.options.num-threads=12
spring.ai.ollama.chat.options.temperature=0.3
spring.ai.ollama.chat.options.top-p=0.9
spring.ai.ollama.chat.options.top-k=40
spring.ai.ollama.chat.options.repeat-penalty=1.1

spring.ai.ollama.client.connect-timeout=120s
spring.ai.ollama.client.read-timeout=600s

spring.ai.ollama.client.max-in-memory-size=50MB

# Enable response streaming for better perceived performance
spring.ai.ollama.chat.options.stream=true

# Swagger UI Configuration
springdoc.api-docs.path=/api-docs
springdoc.swagger-ui.path=/swagger-ui.html
springdoc.swagger-ui.tagsSorter=alpha
springdoc.swagger-ui.operationsSorter=alpha

# Enable debug logging to troubleshoot
logging.level.org.springframework.ai=DEBUG
logging.level.com.srllc.spring_ai_app=DEBUG